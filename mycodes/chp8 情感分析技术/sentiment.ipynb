{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert DeNiro plays the most unbelievably intelligent illiterate of all time. This movie is so wasteful of talent, it is truly disgusting. The script is unbelievable. The dialog is unbelievable. Jane Fonda's character is a caricature of herself, and not a funny one. The movie moves at a snail's pace, is photographed in an ill-advised manner, and is insufferably preachy. It also plugs in every cliche in the book. Swoozie Kurtz is excellent in a supporting role, but so what?<br /><br />Equally annoying is this new IMDB rule of requiring ten lines for every review. When a movie is this worthless, it doesn't require ten lines of text to let other readers know that it is a waste of time and tape. Avoid this movie.\n"
     ]
    }
   ],
   "source": [
    "# with open('../../chapter-8/sentiment-analysis/neg/1_1.txt') as f:\n",
    "#     print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入word列表\n",
      "载入文本向量\n",
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "wordsList = np.load('../../chapter-8/sentiment-analysis/wordsList.npy')\n",
    "print('载入word列表')\n",
    "wordsList = wordsList.tolist()\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]\n",
    "wordVectors = np.load('../../chapter-8/sentiment-analysis/wordVectors.npy')\n",
    "print('载入文本向量')\n",
    "\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.3302e-01,  4.5885e-01,  2.6602e-01,  3.8204e-02,  5.6694e-01,\n",
       "       -5.1194e-01, -1.7831e+00,  4.3072e-01,  1.3246e-03,  1.7852e-02,\n",
       "       -5.1549e-01, -5.4466e-01, -2.5115e-01, -3.7632e-01,  4.0969e-01,\n",
       "        7.0356e-02, -2.6120e-01, -2.7769e-03, -5.1361e-01,  2.9828e-01,\n",
       "        1.4496e-01,  5.3748e-01, -6.2443e-01,  4.1379e-01, -2.2006e-01,\n",
       "       -1.6277e+00,  2.9026e-01, -1.8530e-02,  5.1004e-01, -6.1231e-01,\n",
       "        3.2380e+00,  7.4873e-01,  1.8835e-01,  2.2167e-02,  5.7824e-01,\n",
       "        5.2022e-01,  1.5287e-01,  4.1601e-01,  5.5260e-01, -4.0373e-01,\n",
       "       -1.8057e-01,  2.0914e-01, -2.3873e-01,  1.1284e-01, -7.4940e-03,\n",
       "        5.7918e-01, -4.3641e-01, -8.0904e-01,  2.0410e-01, -1.9532e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_index = wordsList.index('home')\n",
    "wordVectors[home_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正面评价完结\n",
      "负面评价完结\n",
      "文件总数 25000\n",
      "所有的词的数量 5844680\n",
      "平均文件词的长度 233.7872\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "pos_files = ['../../chapter-8/sentiment-analysis/pos/' + f\n",
    "            for f in os.listdir('../../chapter-8/sentiment-analysis/pos/')\n",
    "            if isfile(join('../../chapter-8/sentiment-analysis/pos/', f))]\n",
    "neg_files = ['../../chapter-8/sentiment-analysis/neg/' + f\n",
    "            for f in os.listdir('../../chapter-8/sentiment-analysis/neg/')\n",
    "            if isfile(join('../../chapter-8/sentiment-analysis/neg/', f))]\n",
    "num_words = []\n",
    "for pf in pos_files:\n",
    "    with open(pf, 'r', encoding='UTF-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        num_words.append(counter)\n",
    "print('正面评价完结')\n",
    "\n",
    "for nf in neg_files:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        num_words.append(counter)\n",
    "print('负面评价完结')\n",
    "\n",
    "num_files = len(num_words)\n",
    "print('文件总数', num_files)\n",
    "print('所有的词的数量', sum(num_words))\n",
    "print('平均文件词的长度', sum(num_words) / len(num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use('qt4agg')\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "%matplotlib inline\n",
    "matplotlib.pyplot.hist(num_words, 50, facecolor='g')\n",
    "matplotlib.pyplot.xlabel('文本长度')\n",
    "matplotlib.pyplot.ylabel('频次')\n",
    "matplotlib.pyplot.axis([0, 1200, 0, 8000])\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6fca3c571dfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_count\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'oneand' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6fca3c571dfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_count\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_count\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m399999\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mindexCounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "strip_special_chars = re.compile('[^A-Za-z0-9 ]+')\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace('<br />', ' ')\n",
    "    return re.sub(strip_special_chars, '', string.lower())\n",
    "\n",
    "max_seq_num = 300\n",
    "ids = np.zeros((num_files, max_seq_num), dtype='int32')\n",
    "file_count = 0\n",
    "for pf in pos_files:\n",
    "    with open(pf, 'r', encoding='UTF-8') as f:\n",
    "        indexCounter = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[file_count][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[file_count][indexCounter] = 399999\n",
    "            indexCounter += 1\n",
    "            if indexCounter >= max_seq_num:\n",
    "                break\n",
    "        file_count += 1\n",
    "\n",
    "for nf in neg_files:\n",
    "    with open(nf, 'r', encoding='UTF-8') as f:\n",
    "        indexCounter = 0\n",
    "        line = f.readline()\n",
    "        cleanedLine - cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[file_count][indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                ids[file_count][indexCounter] = 399999\n",
    "            indexCounter += indexCounter\n",
    "            if indexCounter >= max_seq_num:\n",
    "                break\n",
    "        file_count += 1\n",
    "\n",
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def get_train_batch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, max_seq_num])\n",
    "    for i in range(batch_size):\n",
    "        if(i % 2 == 0):\n",
    "            num = randint(1, 11499)\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            num = randint(13499, 24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1: num]\n",
    "    return arr, labels\n",
    "\n",
    "def get_test_batch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batch_size, max_seq_num])\n",
    "    for i in range(batch_size):\n",
    "        num = randint(11499, 13499)\n",
    "        if(num <= 12499):\n",
    "            labels.append([1, 0])\n",
    "        else:\n",
    "            labels.append([0, 1])\n",
    "        arr[i] = ids[num-1: num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "lstm_units = 64\n",
    "num_labels = 2\n",
    "iterations = 100\n",
    "lr = 0.001\n",
    "num_dimensions = 300  # Dimensions for each word vector\n",
    "ids = np.load('../../chapter-8/sentiment-analysis/idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batch_size, num_labels])\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, max_seq_num])\n",
    "data = tf.Variable(\n",
    "    tf.zeros([batch_size, max_seq_num, num_dimensions]), dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookupk(wordVectors, input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.5)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstm_units, num_labels]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.exists('models') and os.path.exists('models/checkpoint'):\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "    else:\n",
    "        if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "            init = tf.initialize_all_variables()\n",
    "        else:\n",
    "            init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "    \n",
    "    iterations = 100\n",
    "    for step in range(iterations):\n",
    "        next_batch, next_batch_labels = get_test_batch()\n",
    "        if step % 20 == 0:\n",
    "            print(\"step:\", step, \" 正确率:\", (sess.run(\n",
    "                accuracy, {input_data: next_batch, labels: next_batch_labels})) * 100)\n",
    "            \n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
