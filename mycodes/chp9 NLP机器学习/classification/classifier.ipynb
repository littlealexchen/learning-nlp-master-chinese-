{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    '''\n",
    "    获取数据\n",
    "    :return: 文本数据，对应的labels\n",
    "    '''\n",
    "    with open('../../../chapter-9/classification/data/ham_data.txt', 'r', encoding='UTF-8') as ham_f, \\\n",
    "    open('../../../chapter-9/classification/data/spam_data.txt', 'r', encoding='UTF-8') as spam_f:\n",
    "        ham_data = ham_f.readlines()\n",
    "        spam_data = spam_f.readlines()\n",
    "        \n",
    "        ham_label = np.ones(len(ham_data)).tolist()\n",
    "        spam_label = np.zeros(len(spam_data)).tolist()\n",
    "        \n",
    "        corpus = ham_data + spam_data\n",
    "        labels = ham_label + spam_label\n",
    "        \n",
    "    return corpus, labels\n",
    "\n",
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    '''\n",
    "    :param corpus: 文本数据\n",
    "    :param labels: label数据\n",
    "    :param test_data_proportion:测试数据占比 \n",
    "    :return: 训练数据,测试数据，训练label,测试label\n",
    "    '''\n",
    "    train_X, test_X, train_Y, test_Y = \\\n",
    "    train_test_split(corpus, labels, test_size=test_data_proportion, random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('accuracy: ', np.round(metrics.accuracy_score(\n",
    "        true_labels, predicted_labels), 2))\n",
    "    print('precision: ', np.round(metrics.precision_score(\n",
    "        true_labels,predicted_labels, average='weighted'),2))\n",
    "    print('recall: ', np.round(metrics.recall_score(\n",
    "        true_labels, predicted_labels, average='weighted'), 2))\n",
    "    print('F1: ', np.round(metrics.f1_score(\n",
    "        true_labels, predicted_labels, average='weighted'), 2))\n",
    "\n",
    "def train_predict_evaluate_model(classifier, train_features, train_labels,\n",
    "                                test_features, test_labels):\n",
    "    # build model\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features)\n",
    "    # evaluate model prediction performance\n",
    "    get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的数据量:  10001\n",
      "样本之一:  北京售票员可厉害，嘿嘿，有专座的，会直接拉着脖子指着鼻子让上面的人站起来让 座的，呵呵，比较赞。。。 杭州就是很少有人给让座，除非司机要求乘客那样做。 五一去杭州一个景点玩，车上有两个不到一岁的小孩，就是没有人给让座，没办法家长只能在车上把小孩的推车打开让孩子坐进去，但是孩子还是闹，只能抱着，景点离市区很远，车上很颠，最后家长坐在地上抱孩子，就是没有一个人给让座，要是在北京，一上车就有人让座了\n",
      "\n",
      "样本的label:  1.0\n",
      "实际类型:  正常邮件\n"
     ]
    }
   ],
   "source": [
    "corpus, labels = get_data()\n",
    "print('总的数据量: ', len(labels))\n",
    "    \n",
    "corpus, labels = remove_empty_docs(corpus, labels)\n",
    "    \n",
    "print('样本之一: ', corpus[10])\n",
    "print('样本的label: ', labels[10])\n",
    "label_name_map = ['垃圾邮件', '正常邮件']\n",
    "print('实际类型: ', label_name_map[int(labels[10])])\n",
    "    \n",
    "train_corpus, test_corpus, train_labels, test_labels = \\\n",
    "prepare_datasets(corpus, labels, test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization import normalize_corpus\n",
    "    \n",
    "# 进行归一化\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)\n",
    "    \n",
    "#     ''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10156', '说', '的', '呵呵', '标题', 'Re', '我', '要是', '你', '女朋友', '绝对', '跟', '你', '分手', 'Re', '昨晚', '猫', '又', '闹', '了', '一夜', '嗯', '，', '谁', '说', '我', '养猫', '是因为', '有', '爱心', '我', '跟', '谁', '急', '是', '哈', '，', '不是', '用', '爱心', '区分', '的', '喜欢', '宠物', '的', '就', '养', '，', '不', '喜欢', '的', '就', '不养', '呗', '卷', '卷', '，', '你', '搞', '成', '这', '副', '鬼', '样子', '，', '还', '好意思', '来', '找', '我', '撒娇', '。', '。', '。'], ['中信', '（', '国际', '）', '电子科技', '有限公司', '推出', '新', '产品', '：', '升职', '步步高', '、', '做生意', '发大财', '、', '连', '找', '情人', '都', '用', '的', '上', '，', '详情', '进入', '网址', 'httpwwwusa5588comccc', '电话', '：', '02033770208', '服务', '热线', '：', '013650852999'], ['贵', '公司', '负责人', '：', '你好', '！', '本', '公司', '祥泰', '实业', '有限公司', '）', '具有', '进出口', '及', '国内贸易', '的', '企业', '承', '多家', '公司', '委托', '有', '广告', '建筑工程', '其它', '服务', '商品销售', '等', '的', '发票', '向', '外代', '开', '点数', '优惠', '本', '公司', '原则', '是', '满意', '后', '付款', '有意者', '请来', '电', '洽谈', '电话', '：', '013631690076', '邮箱', '：', 'shitailong8163com', '联系人', '：', '郭生', '如', '给', '贵', '公司', '带来', '不便', '请谅解'], ['李敖来', '大陆', '，', '轰轰烈烈', '，', '热热闹闹', '，', '国内', '有些', '人', '也', '坐不住', '了', '，', '总', '有人', '要', '跳', '出来', '显摆', '显摆', '，', '不过', '，', '没显', '好', '倒', '是', '现', '了', '眼', '。', '大家', '看看', '下面', '的', '材料', '，', '真', '为', '清华', '汗颜', '啊', '！', '资料', '一', '【', '清华大学', '学者', '评李敖', '演讲', '：', '李敖', '老', '矣', '尚能', '骂否', '？', '】', 'httpnewsanhuinewscomsystem20050923001359039shtml'], ['公司', '名称', '：', '北京', '优力', '维尔', '科技', '有限公司', '高科技', '公司', '招聘', '行政助理', '兼', '前台', '一名', '要求', '：', '25', '岁', '以下', '，', '形象', '良好', '，', '有', '责任心', '、', '诚实', '守信', '、', '有', '团队', '意识', '、', '敬业', '，', '可以', '尽快', '就位', '工作', '地点', '：', '中信', '国安', '数码港', '，', '地理位置', '在', '稻香', '园桥', '桥东', '，', '中国航天', '大厦', '西边', '待遇', '面议', '联系方式', '：', '简历', 'mailtowuwei99tsinghuaorgcn', '公司', '的', '新', '网站', '和', 'mail', '服务器', '都', '在建设中', '，', '投递', '的', '简历', '中', '最好', '附带', '照片', '联系电话', '：', '82652066', '工作日', '9001800', '欢迎', '投递', '简历', '，', '合则', '约见'], ['如果', '他', '只是', '想', '找个', '上床', '的', '，', '这招', '可能', '管用', '，', '但', '对', '他', '身边', '的', '女人', '，', '这招', '不', '适用', '，', '聪明', '的', '老板', '不吃', '窝边草', '。', '是否', '只', '上床', '从来不', '由', '男人', '单方面', '说了算', '。', '如果', '是', '的话', '，', '呵呵', '，', '恐怕', '结婚', '率会', '大大降低', '的', '。', '窝边草', '心理', '是', '个', '阻碍', '，', '克服', '它', '就是', '了', '。', '个人', '猜测', '，', '这种', '人', '一般', '喜欢', '能', '给', '他', '安全感', '的', '女人', '，', '即', '传统', '的', '贤妻良母', '。', '至少', '，', '他们', '更', '倾向', '于', '找', '此类', '女人', '做', '老婆', '。', '即使', '他们', '骨子里', '喜欢', '风骚', '的', '女人', '，', '呵呵', '。', '这个', '猜测', '才', '没道理', '。', '你', '在', '毫无道理', '地', '给', '楼主', '泄气', '。'], ['家园网', '提供', '12M', '免费', '主页', '空间', '，', '欢迎', '申请', '家园网', '空间', '特点', '：', '独立', '二级域名', '，', 'Web', '上传', '，', '马上', '申请', '立即', '开通', '，', '永久', '免费', '终生', '稳定', '安全', '无广告', '客服', '在线', '答疑', '。', '网址', 'httpwwwjayacn', '客服', '信箱', 'geduoyeahnet', '网络实名', '：', '家园网', '家园网', '客服', '中心'], ['中信', '（', '国际', '）', '电子科技', '有限公司', '推出', '新', '产品', '：', '升职', '步步高', '、', '做生意', '发大财', '、', '连', '找', '情人', '都', '用', '的', '上', '，', '详情', '进入', '网址', 'httpwwwusa5588comccc', '电话', '：', '02033770208', '服务', '热线', '：', '013650852999']]\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors import bow_extractor, tfidf_extractor\n",
    "import gensim\n",
    "import jieba\n",
    "    \n",
    "# 词袋模型特征\n",
    "bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "# tfidf 特征\n",
    "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "# tokenize documents\n",
    "tokenized_train = [jieba.lcut(text) for text in norm_train_corpus]\n",
    "print(tokenized_train[2:10])\n",
    "tokenized_test = [jieba.lcut(text) for text in norm_test_corpus]\n",
    "    \n",
    "# build word2vec 模型\n",
    "model = gensim.models.Word2Vec(tokenized_train, size=500, window=100,\n",
    "                                min_count=30, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于词袋模型特征的贝叶斯分类器\n",
      "accuracy:  0.79\n",
      "precision:  0.85\n",
      "recall:  0.79\n",
      "F1:  0.78\n",
      "基于词袋模型特征的逻辑回归\n",
      "accuracy:  0.96\n",
      "precision:  0.96\n",
      "recall:  0.96\n",
      "F1:  0.96\n",
      "基于词袋模型的支持向量机\n",
      "accuracy:  0.97\n",
      "precision:  0.97\n",
      "recall:  0.97\n",
      "F1:  0.97\n",
      "基于tfidf的贝叶斯模型\n",
      "accuracy:  0.79\n",
      "precision:  0.85\n",
      "recall:  0.79\n",
      "F1:  0.78\n",
      "基于tfidf的逻辑回归模型\n",
      "accuracy:  0.94\n",
      "precision:  0.94\n",
      "recall:  0.94\n",
      "F1:  0.94\n",
      "基于tfidf的支持向量机模型\n",
      "accuracy:  0.97\n",
      "precision:  0.97\n",
      "recall:  0.97\n",
      "F1:  0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100) # 书上用的是n_iter会报错\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# 基于词袋模型的多项朴素贝叶斯\n",
    "print(\"基于词袋模型特征的贝叶斯分类器\")\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                   train_features=bow_train_features,\n",
    "                                                   train_labels=train_labels,\n",
    "                                                   test_features=bow_test_features,\n",
    "                                                   test_labels=test_labels)\n",
    "\n",
    "# 基于词袋模型特征的逻辑回归\n",
    "print(\"基于词袋模型特征的逻辑回归\")\n",
    "lr_bow_predictions = train_predict_evaluate_model(classifier=lr,\n",
    "                                                  train_features=bow_train_features,\n",
    "                                                  train_labels=train_labels,\n",
    "                                                  test_features=bow_test_features,\n",
    "                                                  test_labels=test_labels)\n",
    "\n",
    "# 基于词袋模型的支持向量机方法\n",
    "print(\"基于词袋模型的支持向量机\")\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                   train_features=bow_train_features,\n",
    "                                                   train_labels=train_labels,\n",
    "                                                   test_features=bow_test_features,\n",
    "                                                   test_labels=test_labels)\n",
    "\n",
    "\n",
    "# 基于tfidf的多项式朴素贝叶斯模型\n",
    "print(\"基于tfidf的贝叶斯模型\")\n",
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n",
    "# 基于tfidf的逻辑回归模型\n",
    "print(\"基于tfidf的逻辑回归模型\")\n",
    "lr_tfidf_predictions=train_predict_evaluate_model(classifier=lr,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)\n",
    "\n",
    "\n",
    "# 基于tfidf的支持向量机模型\n",
    "print(\"基于tfidf的支持向量机模型\")\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                                     train_features=tfidf_train_features,\n",
    "                                                     train_labels=train_labels,\n",
    "                                                     test_features=tfidf_test_features,\n",
    "                                                     test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "中信（国际）电子科技有限公司推出新产品： 升职步步高、做生意发大财、连找情人都用的上，详情进入 网  址:  http://www.usa5588.com/ccc 电话：020-33770208   服务热线：013650852999 \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "您好！ 我公司有多余的发票可以向外代开！（国税、地税、运输、广告、海关缴款书）。 如果贵公司（厂）有需要请来电洽谈、咨询！ 联系电话: 013510251389  陈先生 谢谢 顺祝商祺! \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "如果您在信箱中不能正常阅读此邮件，请点击这里 \n",
      "邮件类型: 垃圾邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "以下不能正确显示请点此 IFRAME: http://bbs.ewzw.com/viewthread.php?tid=3790 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "你好，我30，未婚，研究生毕业，想找一个在校的女孩做朋友，每月可以给她2000元零用 我不会走入她的生活中去，为她保密 qq ******* 回了一封信以后： 阿，不愿意跟在一群人后面买单，那会让人觉得特傻 只想一对一交往 【 在 leeann2002 的来信中提到: 】 啊,那您介意找一堆女孩子做朋友吗? 也不用给零用钱拉~~用来带我们一起玩就可以了~,唱歌跳舞什么的~~ (要不2000就把自己卖了...) 有照片连接吗? \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "公司：集制作，创作以及宣传为一体的音乐制作文化发展公司 拥有录音棚，主要制作唱片， 也涉及电影、电视剧、广告等音乐制作及创作 职位：经理助理（说文秘也行） 要求：女   22～25 聪明本分 有一定的协调和办事能力 长相过得去 工作业务范围： 平时管理公司文件 接待、电话、上网 负责安排歌手及制作人的工作下达 待遇：试用期月薪1000 转正1500～2000，另有提成 公司地址：北京市朝阳区麦子店 电话留了会被封么？ 先信箱联系吧 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "现已确定2006年度不会进行研究生培养机制改革试点，我校研究生招生类别、培养费用 等相关政策仍按现行规定执行，即我校绝大多数研究生属于国家计划内非定向培养生和 定向培养生，培养费由国家财政拨款。少数研究生（主要是除临床医学以外的专业学位 硕士生）属于委托培养生（培养费由选送单位支付）或自筹经费培养生（培养费由考生 本人自筹），其交纳培养费的标准详见我校财务处（网址：http://10.49.99.99）的公 示（2005.9.8）。 \n",
      "邮件类型: 正常邮件\n",
      "预测的邮件类型: 垃圾邮件\n",
      "文本:-\n",
      "提前征友K歌，只要是MM，但是还是有点小要求，age&lt;=23岁，相貌不在乎，身高不限。 K歌地点暂时选在交通大学附近的佰金KTV，因为ME手头有两张优惠券。 那里环境虽然比不上PARTYWORLD，但是比Melody稍微好那么一点点。 如果想报名的人请联系本人。 QQ：275738585 MSN：gao_520@hotmail.com 加我时请务必注明 SMTH 如果有意的MM，请联系我，组织好了大家以后，星期六午饭就在KTV吃。 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 0:\n",
    "        print('邮件类型:', label_name_map[int(label)])\n",
    "        print('预测的邮件类型:', label_name_map[int(predicted_label)])\n",
    "        print('文本:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 1 and predicted_label == 0:\n",
    "        print('邮件类型:', label_name_map[int(label)])\n",
    "        print('预测的邮件类型:', label_name_map[int(predicted_label)])\n",
    "        print('文本:-')\n",
    "        print(re.sub('\\n', ' ', document))\n",
    "\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 草稿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讲的是孔子后人的故事。一个老领导回到家乡，跟儿子感情不和，跟贪财的孙子孔为本和睦。 老领导的弟弟魏宗万是赶马车的。 有个洋妞大概是考察民俗的，在他们家过年。 孔为本总想出国，被爷爷教育了。 最后，一家人基本和解。 顺便问另一类电影，北京青年电影制片厂的。中越战背景。一军人被介绍了一个对象，去相亲。女方是军队医院的护士，犹豫不决，总是在回忆战场上负伤的男友，好像还没死。最后 男方表示理解，归队了。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../../chapter-9/classification/data/ham_data.txt', encoding='UTF-8') as ham_f:\n",
    "    print(ham_f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.cut at 0x000002141D3DC0C0>\n",
      "['我', '最近', '很', '好', '~', '！', '@', '你', '呢', '？']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "filter expected 2 arguments, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1fec135daf47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mremove_special_characters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'我最近很好~！@你呢？'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-1fec135daf47>\u001b[0m in \u001b[0;36mremove_special_characters\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[{}]'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print([pattern.sub('', token) for token in tokens])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mfiltered_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#     filtered_text = ' '.join(filtered_tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#     return filtered_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: filter expected 2 arguments, got 1"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "import string\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "def tokenize_text(text):\n",
    "    tokens = jieba.cut(text)\n",
    "    print(tokens)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "remove_special_characters('我最近很好~！@你呢？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
