{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Alex\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.597 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000articles\n",
      "Saved 20000articles\n",
      "Saved 30000articles\n",
      "Saved 40000articles\n",
      "Saved 50000articles\n",
      "Saved 60000articles\n",
      "Saved 70000articles\n",
      "Saved 80000articles\n",
      "Saved 90000articles\n",
      "Saved 100000articles\n",
      "Saved 110000articles\n",
      "Saved 120000articles\n",
      "Saved 130000articles\n",
      "Saved 140000articles\n",
      "Saved 150000articles\n",
      "Saved 160000articles\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mdata_pre_process\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\alex_space\\software\\anaconda\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mcut\u001b[1;34m(self, sentence, cut_all, HMM)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\alex_space\\software\\anaconda\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mDAG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_DAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mroute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDAG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\alex_space\\software\\anaconda\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mcalc\u001b[1;34m(self, sentence, DAG, route)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n\u001b[1;32m--> 176\u001b[1;33m                               logtotal + route[x + 1][0], x) for x in DAG[idx])\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_DAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\alex_space\\software\\anaconda\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n\u001b[1;32m--> 176\u001b[1;33m                               logtotal + route[x + 1][0], x) for x in DAG[idx])\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_DAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.corpora import WikiCorpus\n",
    "import jieba\n",
    "from langconv import *\n",
    "\n",
    "def data_pre_process():\n",
    "    space = ' '\n",
    "    i = 0\n",
    "    l = []\n",
    "    zhwiki_name = './data/zhwiki-latest-pages-articles.xml.bz2'\n",
    "    f = open('./data/reduce_zhiwiki.txt', 'w', encoding='utf-8')\n",
    "    wiki = WikiCorpus(zhwiki_name, lemmatize=False, dictionary={})\n",
    "    for text in wiki.get_texts():\n",
    "        for temp_sentence in text:\n",
    "            temp_sentence = Converter('zh-hans').convert(temp_sentence)\n",
    "            seg_list = list(jieba.cut(temp_sentence))\n",
    "            for temp_term in seg_list:\n",
    "                l.append(temp_term)\n",
    "            f.write(space.join(l) + '\\n')\n",
    "            l = []\n",
    "            i = i + 1\n",
    "            \n",
    "            if(i % 10000 == 0):\n",
    "                print('Saved ' + str(i) + 'articles')\n",
    "    f.close()\n",
    "if __name__ == '__main__':\n",
    "    data_pre_process()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-06 08:34:13,217 : INFO : collecting all words and their counts\n",
      "2019-09-06 08:34:13,218 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-09-06 08:34:13,274 : INFO : PROGRESS: at sentence #10000, processed 26277 words, keeping 6872 word types\n",
      "2019-09-06 08:34:13,324 : INFO : PROGRESS: at sentence #20000, processed 57739 words, keeping 12696 word types\n",
      "2019-09-06 08:34:13,366 : INFO : PROGRESS: at sentence #30000, processed 89133 words, keeping 19776 word types\n",
      "2019-09-06 08:34:13,421 : INFO : PROGRESS: at sentence #40000, processed 120877 words, keeping 25523 word types\n",
      "2019-09-06 08:34:13,477 : INFO : PROGRESS: at sentence #50000, processed 150943 words, keeping 31160 word types\n",
      "2019-09-06 08:34:13,526 : INFO : PROGRESS: at sentence #60000, processed 181935 words, keeping 34876 word types\n",
      "2019-09-06 08:34:13,578 : INFO : PROGRESS: at sentence #70000, processed 208938 words, keeping 38771 word types\n",
      "2019-09-06 08:34:13,631 : INFO : PROGRESS: at sentence #80000, processed 235660 words, keeping 42612 word types\n",
      "2019-09-06 08:34:13,685 : INFO : PROGRESS: at sentence #90000, processed 261001 words, keeping 46456 word types\n",
      "2019-09-06 08:34:13,737 : INFO : PROGRESS: at sentence #100000, processed 288241 words, keeping 49660 word types\n",
      "2019-09-06 08:34:13,792 : INFO : PROGRESS: at sentence #110000, processed 319518 words, keeping 53121 word types\n",
      "2019-09-06 08:34:13,845 : INFO : PROGRESS: at sentence #120000, processed 349840 words, keeping 55834 word types\n",
      "2019-09-06 08:34:13,901 : INFO : PROGRESS: at sentence #130000, processed 381430 words, keeping 59241 word types\n",
      "2019-09-06 08:34:13,955 : INFO : PROGRESS: at sentence #140000, processed 412261 words, keeping 62286 word types\n",
      "2019-09-06 08:34:14,009 : INFO : PROGRESS: at sentence #150000, processed 445281 words, keeping 64749 word types\n",
      "2019-09-06 08:34:14,063 : INFO : PROGRESS: at sentence #160000, processed 472211 words, keeping 67868 word types\n",
      "2019-09-06 08:34:14,068 : INFO : collected 68021 word types from a corpus of 475458 raw words and 160918 sentences\n",
      "2019-09-06 08:34:14,068 : INFO : Loading a fresh vocabulary\n",
      "2019-09-06 08:34:14,129 : INFO : min_count=5 retains 12157 unique words (17% of original 68021, drops 55864)\n",
      "2019-09-06 08:34:14,132 : INFO : min_count=5 leaves 390606 word corpus (82% of original 475458, drops 84852)\n",
      "2019-09-06 08:34:14,244 : INFO : deleting the raw counts dictionary of 68021 items\n",
      "2019-09-06 08:34:14,250 : INFO : sample=0.001 downsamples 21 most-common words\n",
      "2019-09-06 08:34:14,251 : INFO : downsampling leaves estimated 353313 word corpus (90.5% of prior 390606)\n",
      "2019-09-06 08:34:14,381 : INFO : estimated required memory for 12157 words and 192 dimensions: 24751652 bytes\n",
      "2019-09-06 08:34:14,386 : INFO : resetting layer weights\n",
      "2019-09-06 08:34:14,718 : INFO : training model with 9 workers on 12157 vocabulary and 192 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-09-06 08:34:15,795 : INFO : EPOCH 1 - PROGRESS: at 33.73% examples, 115849 words/s, in_qsize 17, out_qsize 1\n",
      "2019-09-06 08:34:16,171 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-09-06 08:34:16,190 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-09-06 08:34:16,203 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-09-06 08:34:16,207 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-09-06 08:34:16,220 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-09-06 08:34:16,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-06 08:34:16,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-06 08:34:16,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-06 08:34:16,235 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-06 08:34:16,236 : INFO : EPOCH - 1 : training on 475458 raw words (353334 effective words) took 1.5s, 242141 effective words/s\n",
      "2019-09-06 08:34:17,285 : INFO : EPOCH 2 - PROGRESS: at 47.63% examples, 164849 words/s, in_qsize 12, out_qsize 1\n",
      "2019-09-06 08:34:17,604 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-09-06 08:34:17,610 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-09-06 08:34:17,621 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-09-06 08:34:17,624 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-09-06 08:34:17,625 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-09-06 08:34:17,626 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-06 08:34:17,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-06 08:34:17,631 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-06 08:34:17,639 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-06 08:34:17,641 : INFO : EPOCH - 2 : training on 475458 raw words (353463 effective words) took 1.4s, 253669 effective words/s\n",
      "2019-09-06 08:34:18,676 : INFO : EPOCH 3 - PROGRESS: at 36.78% examples, 129485 words/s, in_qsize 17, out_qsize 0\n",
      "2019-09-06 08:34:18,981 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-09-06 08:34:19,019 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-09-06 08:34:19,055 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-09-06 08:34:19,061 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-09-06 08:34:19,067 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-09-06 08:34:19,073 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-06 08:34:19,076 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-06 08:34:19,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-06 08:34:19,090 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-06 08:34:19,091 : INFO : EPOCH - 3 : training on 475458 raw words (353232 effective words) took 1.4s, 244788 effective words/s\n",
      "2019-09-06 08:34:20,326 : INFO : EPOCH 4 - PROGRESS: at 48.41% examples, 138834 words/s, in_qsize 17, out_qsize 0\n",
      "2019-09-06 08:34:20,498 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-09-06 08:34:20,505 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-09-06 08:34:20,506 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-09-06 08:34:20,541 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-09-06 08:34:20,549 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-09-06 08:34:20,555 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-06 08:34:20,558 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-06 08:34:20,562 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-06 08:34:20,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-06 08:34:20,568 : INFO : EPOCH - 4 : training on 475458 raw words (353315 effective words) took 1.5s, 241021 effective words/s\n",
      "2019-09-06 08:34:21,592 : INFO : EPOCH 5 - PROGRESS: at 45.92% examples, 160682 words/s, in_qsize 11, out_qsize 0\n",
      "2019-09-06 08:34:21,945 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-09-06 08:34:21,971 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-09-06 08:34:21,989 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-09-06 08:34:21,991 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-09-06 08:34:21,992 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-09-06 08:34:21,993 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-06 08:34:21,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-06 08:34:22,020 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-06 08:34:22,024 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-06 08:34:22,026 : INFO : EPOCH - 5 : training on 475458 raw words (353513 effective words) took 1.4s, 244399 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-06 08:34:22,026 : INFO : training on a 2377290 raw words (1766857 effective words) took 7.3s, 241809 effective words/s\n",
      "2019-09-06 08:34:22,028 : INFO : saving Word2Vec object under zhiwiki_news.word2vec, separately None\n",
      "2019-09-06 08:34:22,030 : INFO : not storing attribute vectors_norm\n",
      "2019-09-06 08:34:22,032 : INFO : not storing attribute cum_table\n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-09-06 08:34:22,552 : INFO : saved zhiwiki_news.word2vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def word_2_vec():\n",
    "    wiki_news = open('./data/reduce_zhiwiki.txt', 'r', encoding='utf-8')\n",
    "    model = Word2Vec(LineSentence(wiki_news), sg=0, size=192, window=5, min_count=5, workers=9)\n",
    "    model.save('zhiwiki_news.word2vec')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word_2_vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-06 08:43:12,561 : INFO : loading Word2Vec object from zhiwiki_news.word2vec\n",
      "2019-09-06 08:43:12,885 : INFO : loading wv recursively from zhiwiki_news.word2vec.wv.* with mmap=None\n",
      "2019-09-06 08:43:12,887 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-09-06 08:43:12,887 : INFO : loading vocabulary recursively from zhiwiki_news.word2vec.vocabulary.* with mmap=None\n",
      "2019-09-06 08:43:12,889 : INFO : loading trainables recursively from zhiwiki_news.word2vec.trainables.* with mmap=None\n",
      "2019-09-06 08:43:12,890 : INFO : setting ignored attribute cum_table to None\n",
      "2019-09-06 08:43:12,891 : INFO : loaded zhiwiki_news.word2vec\n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  import sys\n",
      "D:\\alex_space\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "2019-09-06 08:43:13,003 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996130372516774\n",
      "0.9752977770527238\n",
      "0.9788305909243477\n",
      "0.9839468240580983\n",
      "[('上', 0.9830508828163147), ('重要', 0.9821333885192871), ('世界', 0.9812822937965393), ('最', 0.9770735502243042), ('名村', 0.9763994216918945), ('文化', 0.975906491279602), ('一个', 0.9746402502059937), ('摇篮', 0.9731670618057251), ('悠久', 0.9722903370857239), ('研究', 0.9718396067619324)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "def test():\n",
    "    model = gensim.models.Word2Vec.load('zhiwiki_news.word2vec')\n",
    "    print(model.similarity('数学', '科学'))\n",
    "    print(model.similarity('数学', '研究'))\n",
    "    print(model.similarity('数学', '几何'))\n",
    "    print(model.similarity('数学', '数字'))\n",
    "    \n",
    "    word = '中国'\n",
    "    if word in model.wv.index2word:\n",
    "        print(model.most_similar(word))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
